## ==============================
##      Chatbot Configuration
##
##   This file contains the necessary parameters for configuring the chatbot.
##   Please review all sections carefully to ensure proper functionality.
##
##   Sections:
##   1- CHATBOT CONFIGURATION
##   2- LLM CONFIGURATION
##   3- VECTOR STORE CONFIGURATION
##   4- AI SDK CONFIGURATION
## ==============================

# AI_SDK_VER = 0.10.1

## ==============================
## 1. CHATBOT CONFIGURATION
## Set the host, port, and other general chatbot settings.
## ==============================

# CHATBOT_HOST = 0.0.0.0
# CHATBOT_PORT = 9992

# CHATBOT_ROOT_PATH allows the sample chatbot to run on a different path. For example, 
# if using CHATBOT_ROOT_PATH = denodo-ai-sdk-sample-chatbot, the sample chatbot UI will
# be accessible at localhost:9992/denodo-ai-sdk-sample-chatbot/
#CHATBOT_ROOT_PATH = "denodo-ai-sdk-sample-chatbot"

# If you want to activate HTTPS, write down the key and certificate path here.
#CHATBOT_SSL_CERT = cert.pem 
#CHATBOT_SSL_KEY = key.pem

# When deploying to production, you might want to use multiple workers (cores) by setting CHATBOT_WORKERS
# CHATBOT_WORKERS = 1

#CHATBOT_TIMEOUT sets the worker timeout in seconds for production mode (Gunicorn/Uvicorn)
#This is especially important when using the analyst tool which can take 5-15 minutes
#Default is 1200 seconds (20 minutes)
#CHATBOT_TIMEOUT = 1200

#If CHATBOT_REPORTING is set to 1, all the users queries, responses and token usage will be recorded in reports/user_report.csv
#Please set it to 0 to avoid this behaviour.
# CHATBOT_REPORTING = 1

#Max side in megabytes of the reports generated in reports/. Will create a new timestamped one after reaching maxsize.
#Defaults to 10 mb.
# CHATBOT_REPORT_MAX_SIZE=10

#By default, users can add feedback (positive or negative) + details on specific interactions.
#This feedback is added to the report in reports/user_report.csv
# CHATBOT_FEEDBACK = 1

#By default, users can upload their own unstructured data through CSV files and ask questions about their documents.
#Set CHATBOT_UNSTRUCTURED_MODE = 0 to disable it.
# CHATBOT_UNSTRUCTURED_MODE=1

#Allow users to edit LLM settings (provider, model, temperature, max_tokens) through the settings modal.
#Set to 1 to enable LLM editing, 0 to disable. This setting cannot be changed after startup.
# CHATBOT_USER_EDIT_LLM = 0

#Instead of having users upload their own unstructured data through a CSV file, you can connect the chatbot to an already populated
#vector store. Please set CHATBOT_UNSTRUCTURED_MODE to 0 to avoid users overwriting this with their own CSVs.
#CHATBOT_UNSTRUCTURED_INDEX=custom_kb
#CHATBOT_UNSTRUCTURED_DESCRIPTION="Use this knowledge base whenever you receive questions about the Denodo software"

#The chatbot LLM might decide to request a graph generation even when not explicitly asked for in your question.
#To inject the chatbot LLM to only generate a graph when explicitly requested, disable CHATBOT_AUTO_GRAPH=0.
# CHATBOT_AUTO_GRAPH=1

# Frontend timeout (in ms) for the vector store synchronization call.
# CHATBOT_SYNC_VDBS_TIMEOUT = 600000

# Whether to verify SSL certificates for AI SDK requests.
# Set to 1 to enable SSL verification; 0 to disable (default: 0).
# AI_SDK_VERIFY_SSL = 0

# If working in an offline, restricted environment and you need to connect to external endpoints with custom certificates,
# please use the following variables to point to those certificates for SSL verification.
# NOTE: The following path has been provided as reference, please set your own.
#REQUESTS_CA_BUNDLE=/home/denodo9_dev/denodo-ai-sdk-public-9-dev/denodo-ai-sdk-public-9-dev/SampleCA.cer
#SSL_CERT_FILE=/home/denodo9_dev/denodo-ai-sdk-public-9-dev/denodo-ai-sdk-public-9-dev/SampleCA.cer

## ==============================
## 2. LLM/Embeddings CONFIGURATION
## Specify the LLM provider, model, and API key for both LLM and embeddings model
## ==============================

# CHATBOT_LLM_PROVIDER = openai
# CHATBOT_LLM_MODEL = gpt-4o
# CHATBOT_EMBEDDINGS_PROVIDER = openai
# CHATBOT_EMBEDDINGS_MODEL = text-embedding-3-large

# Maximum number of rows from the VQL execution result that are passed to the LLM for generating the final answer.
#CHATBOT_LLM_RESPONSE_ROWS_LIMIT=15

##==============================
## Ollama
## There is no specific configuration for Ollama, but:
##      - You must have the model already installed first through Ollama
##      - You must use the same model ID in CHAT_MODEL and SQL_GENERATION_MODEL as the one you use in Ollama
## There's no need to execute 'ollama run <model-id>' to use it in the AI SDK.
## The SDK will automatically attempt to connect to the default Ollama base URL and port, but
## you can modify this using the parameter OLLAMA_API_BASE_URL.
##==============================

#OLLAMA_API_BASE_URL = http://localhost:11434

##==============================
## OpenAI
## If you want to have two different OpenAI-API compatible providers, please check the user manual.
##==============================

# OPENAI_API_KEY defines the API key for your OpenAI account.

# OPENAI_API_KEY = 

# OpenAI base url can be set OPTIONALLY if using a OpenAI-compatible provider different from OpenAI.

#OPENAI_BASE_URL = 

# OpenAI proxy to use. Set as http://{user}:{pwd}@{host}:{port} format

#OPENAI_PROXY_URL = 

# OpenAI organization ID. If not set it will use the default.
#OPENAI_ORG_ID = 

# You can set the dimensions (size of the vector object) with this variable for the embeddings model.
# It is recommended to leave it to the model's default dimension size.

#OPENAI_EMBEDDINGS_DIMENSIONS = 

##==============================
## AzureOpenAI
## Please set the deployment name in the CHAT_MODEL/SQL_GENERATION_MODEL variables.
## The model (deployment) name used will be appended to the Azure OpenAI endpoint, like this:
## /deployments/{CHAT_MODEL}
##==============================

# AZURE_OPENAI_ENDPOINT and AZURE_API_VERSION define the connection string and version to your AzureOpenAI instance.
# AZURE_OPENAI_ENDPOINT refers to the everything until the azure.com domain. For example: https://example-resource.openai.azure.com/
# The AI SDK will automatically append /openai/deployments/{model_name}/chat/completions... to the endpoint.

# AZURE_OPENAI_ENDPOINT = 
# AZURE_API_VERSION = 

##AZURE_OPENAI_API_KEY defines the API key for your OpenAI account.

# AZURE_OPENAI_API_KEY = 

# AzureOpenAI proxy to use. Set as http://{user}:{pwd}@{host}:{port} format

#AZURE_OPENAI_PROXY = 

# You can set the dimensions (size of the vector object) with this variable for the embeddings model.
# It is recommended to leave it to the model's default dimension size.

#AZUREOPENAI_EMBEDDINGS_DIMENSIONS = 

##==============================
## Google
## NOTE: This is Google Cloud's Vertex AI service. Meant for production.
## A JSON service account with permissions is needed as application credentials.
##==============================

# GOOGLE_APPLICATION_CREDENTIALS defines the path to the JSON storing your Google Application Credentials

# GOOGLE_APPLICATION_CREDENTIALS = 

# Enable Google's extended thinking mode for better reasoning capabilities.
# Set to 1 to enable thinking mode, 0 to disable.
#GOOGLE_THINKING = 0

# Set the number of tokens allocated for Google's thinking process.
# This controls how much the model can "think" before generating the final response.
# Higher values allow for more thorough reasoning but consume more tokens.
#GOOGLE_THINKING_TOKENS = 2000 

##==============================
## Google AI Studio
## NOTE: Not intended for production
##==============================

# GOOGLE_AI_STUDIO_API_KEY is your Google AI Studio API key

# GOOGLE_AI_STUDIO_API_KEY = 

##==============================
## Groq
##==============================

# GROQ_API_KEY defines the API key for your OpenAI account.

# GROQ_API_KEY = 

##==============================
## OpenRouter
##==============================

# OPENROUTER_API_KEY defines the API key for your OpenRouter account.

# OPENROUTER_API_KEY = 

# In a comma-separated list, you can specify the providers you prefer OpenRouter route your LLM calls to.
#OPENROUTER_PREFERRED_PROVIDERS = 

##==============================
## SambaNova
##==============================

# SAMBANOVA_API_KEY defines the API key for your SambaNova account.

# SAMBANOVA_API_KEY = 

##==============================
## Bedrock
##==============================

# AWS_REGION, AWS_PROFILE_NAME, AWS_ROLE_ARN, AWS_SECRET_ACCESS_KEY, and AWS_ACCESS_KEY_ID define the connection parameters to your AWS Bedrock instance.
# If using EC2 with an IAM profile, you only need to set AWS_REGION to select the region you want to deploy Bedrock in.
# This is relevant because different regions have different models/costs/latencies and it is a required parameter by AWS when making a call.

# AWS_REGION = 
# AWS_PROFILE_NAME = 
# AWS_ROLE_ARN = 
# AWS_ACCESS_KEY_ID = 
# AWS_SECRET_ACCESS_KEY = 

# By default, AWS SDK use the global STS endpoint (https://sts.amazonaws.com).
# To enable region-specific STS endpoints (e.g., https://sts.us-west-2.amazonaws.com),
# uncomment the following line:
#AWS_STS_REGIONAL_ENDPOINTS=regional

# Enable Claude's extended thinking mode for better reasoning capabilities.
# Set to 1 to enable thinking mode, 0 to disable.
# Note: Only available for specific Claude models:
#   - Claude Opus 4: anthropic.claude-opus-4-20250514-v1:0
#   - Claude Sonnet 4: anthropic.claude-sonnet-4-20250514-v1:0
#   - Claude 3.7 Sonnet: anthropic.claude-3-7-sonnet-20250219-v1:0
#AWS_CLAUDE_THINKING = 0

# Set the number of tokens allocated for Claude's thinking process.
# This controls how much the model can "think" before generating the final response.
# Higher values allow for more thorough reasoning but consume more tokens.
#AWS_CLAUDE_THINKING_TOKENS = 2000

##==============================
## Mistral
##==============================

# MISTRAL_API_KEY = 

##==============================
## NVIDIA NIM
##==============================

# NVIDIA_API_KEY = 

# If self-hosting NVIDIA NIM, set the base url here, like "http://localhost:8000/v1"
#NVIDIA_BASE_URL = 

## ==============================
## 3. VECTOR STORE CONFIGURATION
## Specify the vector store provider.
## Available: Chroma, PGVector, OpenSearch
## ==============================

# CHATBOT_VECTOR_STORE_PROVIDER = chroma

##==============================
## PGVector
##==============================

# The full connection string to the PGVector with username and password. For example: postgresql+psycopg://langchain:langchain@localhost:6024/langchain
# Must include postgresql+psycopg at the beginning. After that it's user:pwd@host:port/db

# PGVECTOR_CONNECTION_STRING = 

##==============================
## OpenSearch
##==============================

# The URL of the OpenSearch instance. Default: http://localhost:9200
# OPENSEARCH_URL =
# OPENSEARCH_USERNAME =
# OPENSEARCH_PASSWORD = 

## ==============================
## 4. AI SDK CONFIGURATION
## Set the connection details for the AI SDK.
## The USERNAME and PASSWORD are needed if you want chatbot users to be able to sync the vector store from the chatbot.
## These credentials must be the credentials of the user with permissions to execute the getMetadata endpoint.
## ==============================

# AI_SDK_URL = http://localhost:8008
# AI_SDK_USERNAME = admin
# AI_SDK_PASSWORD = admin

#Only the AI SDK server has access to the Data Catalog URL.
#If you want to expose it in the chatbot for automatic view linking, include the DC URL here
#CHATBOT_DATA_CATALOG_URL = 