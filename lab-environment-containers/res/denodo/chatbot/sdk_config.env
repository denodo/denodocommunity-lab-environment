## ==============================
## 		Denodo-Gen-AI-SDK
##	    configuration parameters
## 
##   Within this file you will need to set all the necessary parameters for configuring the AI SDK.
##   Carefully go through all the sections to ensure the AI SDK works properly with your environment.
##  
##   1- API CONFIGURATION
##   2- DATA CATALOG CONFIGURATION
##   3- VECTOR STORE CONFIGURATION
##   4- LLM/EMBEDDINGS SELECTION
##   5- LLM/EMBEDDINGS PROVIDER CONFIGURATION
##   6- CUSTOM INSTRUCTIONS
##   7- DEEPQUERY
## ==============================

# AI_SDK_VER = 0.10.1

## ==============================
## 1. API CONFIGURATION
## Establish the host and port the API should listen from.
## ==============================

# AI_SDK_HOST = 0.0.0.0
# AI_SDK_PORT = 8008

# AI_SDK_ROOT_PATH allows the AI SDK to run on a different path. For example, 
# if using AI_SDK_ROOT_PATH = denodo-ai-sdk/v1, the AI SDK will receive requests
# at localhost:8008/denodo-ai-sdk/v1/health
#AI_SDK_ROOT_PATH = "denodo-ai-sdk/v1"

## If you want to activate HTTPS, write down the key and certificate path here.

#AI_SDK_SSL_CERT = 
#AI_SDK_SSL_KEY = 

# By default, the AI SDK runs a single-threaded uvicorn server with 1 worker (thread)
# You can increase the number of workers with the AI_SDK_WORKERS parameter.
#AI_SDK_WORKERS = 4

# You can set SENSITIVE_DATA_LOGGING to 0 to disable the input/output of functions to the logs.
# SENSITIVE_DATA_LOGGING = 1

# AI_SDK_TIMEOUT sets the worker timeout in seconds for production mode (Gunicorn/Uvicorn)
# This is especially important for long-running endpoints like the analyst mode which can take 5-15 minutes
# Default is 1200 seconds (20 minutes)
#AI_SDK_TIMEOUT = 1200

# If working in an offline, restricted environment and you need to connect to external endpoints with custom certificates,
# please use the following variables to point to those certificates for SSL verification.
# NOTE: The following path has been provided as reference, please set your own.
#REQUESTS_CA_BUNDLE=/home/denodo9_dev/denodo-ai-sdk-public-9-dev/denodo-ai-sdk-public-9-dev/SampleCA.cer
#SSL_CERT_FILE=/home/denodo9_dev/denodo-ai-sdk-public-9-dev/denodo-ai-sdk-public-9-dev/SampleCA.cer

## ==============================
## 2. Data Catalog Configuration
## Parameters that determine the connection to the Data Catalog to retrieve the metadata and execute queries.
## Example AI_SDK_DATA_CATALOG_URL: http://localhost:9090/denodo-data-catalog/
## ==============================

# AI_SDK_DATA_CATALOG_URL = http://localhost:9090/denodo-data-catalog/

# DATA_CATALOG_SERVER_ID defines the ID of the VDP connected to the Data Catalog, it defaults to 1 if not set. 
# Only modify this property if you have multiple VDP's connected to your Data Catalog

# DATA_CATALOG_SERVER_ID = 1

# If using SSL, you can set to 0 to not verify or you can include the path of the certificates to verify.

# DATA_CATALOG_VERIFY_SSL = 0

##==============================
## 3.Vector Store configuration
## Select the vector store provider and then the specific configuration for that vector store
## Available: Chroma, PGVector, OpenSearch
##==============================

# VECTOR_STORE is the provider of the vector store

# VECTOR_STORE = chroma

# Use RATE_LIMIT_RPM to set the maximum number of views to be added (and embedded) every minute

#RATE_LIMIT_RPM = 

##==============================
## PGVector
##==============================

# The full connection string to the PGVector with username and password. For example: postgresql+psycopg://langchain:langchain@localhost:6024/langchain
# Must include postgresql+psycopg at the beginning. After that it's user:pwd@host:port/db

# PGVECTOR_CONNECTION_STRING = 

##==============================
## OpenSearch
##==============================

# The URL of the OpenSearch instance. Default: http://localhost:9200
# OPENSEARCH_URL =
# OPENSEARCH_USERNAME =
# OPENSEARCH_PASSWORD = 

## ==============================
## 4. LLM/Embeddings selection
## Fill in the corresponding parameters below.
## Compatible providers:
## OpenAI
## Azure
## Bedrock
## Google
## GoogleAIStudio
## Anthropic
## NVIDIA
## Groq
## Ollama
## Mistral
## SambaNova
## OpenRouter
## + Any OpenAI or Azure compatible endpoint (using Custom Providers)
## Check README for recommended models for each provider.
## ==============================

# LLM_PROVIDER defines which LLM you will be using for basic tasks (chat, SQL generation)

# LLM_PROVIDER = OpenAI

# LLM_MODEL defines the specific model ID that will be using for general tasks.
# Examples: anthropic.claude-3-5-sonnet-20240620-v1:0, gpt-4o, gemini-2.5-flash, etc..

# LLM_MODEL = gpt-4o

# LLM_TEMPERATURE defines the temperature setting for the general LLM (0.0 to 2.0 for OpenAI, 0.0 to 1.0 for others)

# LLM_TEMPERATURE = 0.0

# LLM_MAX_TOKENS defines the maximum tokens for the general LLM. Not recommended to decrease this value.

# LLM_MAX_TOKENS = 2048

# THINKING_LLM_PROVIDER defines which LLM you will be using for thinking/reasoning tasks (DeepQuery planning).

# THINKING_LLM_PROVIDER = OpenAI

# THINKING_LLM_MODEL defines the specific model ID for thinking/reasoning tasks.
# Examples: o4-mini, o3-mini, deepseek-r1, etc.

# THINKING_LLM_MODEL = o4-mini

# THINKING_LLM_TEMPERATURE defines the temperature setting for the thinking LLM.
# Thinking models are sometimes stricter with temperature settings.
# For example, Mistral with Magistral only accepts 0.7 and OpenAI with o4-mini only accepts 1.

# THINKING_LLM_TEMPERATURE = 1.0

# THINKING_LLM_MAX_TOKENS defines the maximum tokens for the thinking LLM. Not recommended to lower this value.

# THINKING_LLM_MAX_TOKENS = 10240

# EMBEDDINGS_PROVIDER defines the specific provider you will be using for the embeddings.

# EMBEDDINGS_PROVIDER = OpenAI

# EMBEDDINGS_MODEL defines the specific model ID that will be using for the embeddings.
# Example: text-embedding-3-large

# EMBEDDINGS_MODEL = text-embedding-3-large

# EMBEDDINGS_TOKEN_LIMIT refers to the maximum input tokens your embedding model can receive.
# Activate this option to activate chunking of views with big schemas.
#EMBEDDINGS_TOKEN_LIMIT = 

# Maximum number of rows to return from the VQL execution result.
#VQL_EXECUTE_ROWS_LIMIT=100

# Maximum number of rows from the VQL execution result that are passed to the LLM for generating the final answer.
#LLM_RESPONSE_ROWS_LIMIT=15

## ==============================
## 5. LLM/Embeddings configuration
## ==============================

##==============================
## Ollama
## There is no specific configuration for Ollama, but:
##      - You must have the model already installed first through Ollama
##      - You must use the same model ID in LLM_MODEL as the one you use in Ollama
## There's no need to execute 'ollama run <model-id>' to use it in the AI SDK.
## The SDK will automatically attempt to connect to the default Ollama base URL and port, but
## you can modify this using the parameter OLLAMA_API_BASE_URL.
##==============================

#OLLAMA_API_BASE_URL = http://localhost:11434

##==============================
## OpenAI
## If you want to have two different OpenAI-API compatible providers, please check the user manual.
##==============================

# OPENAI_API_KEY defines the API key for your OpenAI account.

# OPENAI_API_KEY = 

# OpenAI base url can be set OPTIONALLY if using a OpenAI-compatible provider different from OpenAI.

#OPENAI_BASE_URL = 

# OpenAI proxy to use. Set as http://{user}:{pwd}@{host}:{port} format

#OPENAI_PROXY_URL = 

# OpenAI organization ID. If not set it will use the default.
#OPENAI_ORG_ID = 

# You can set the dimensions (size of the vector object) with this variable for the embeddings model.
# It is recommended to leave it to the model's default dimension size.

#OPENAI_EMBEDDINGS_DIMENSIONS = 

##==============================
## Azure
## Please set the deployment name in the LLM_MODEL variable.
## The model (deployment) name used will be appended to the Azure OpenAI endpoint, like this:
## /deployments/{LLM_MODEL}
##==============================

# AZURE_ENDPOINT and AZURE_API_VERSION define the connection string and version to your Azure instance.
# AZURE_ENDPOINT refers to the everything until the azure.com domain. For example: https://example-resource.openai.azure.com/
# The AI SDK will automatically append /openai/deployments/{model_name}/chat/completions... to the endpoint.

# AZURE_ENDPOINT = 
# AZURE_API_VERSION = 

##AZURE_API_KEY defines the API key for your OpenAI account.

# AZURE_API_KEY = 

# Azure proxy to use. Set as http://{user}:{pwd}@{host}:{port} format

#AZURE_PROXY = 

# You can set the dimensions (size of the vector object) with this variable for the embeddings model.
# It is recommended to leave it to the model's default dimension size.

#AZURE_EMBEDDINGS_DIMENSIONS = 

##==============================
## Google
## NOTE: This is Google Cloud's Vertex AI service. Meant for production.
## A JSON service account with permissions is needed as application credentials.
##==============================

# GOOGLE_APPLICATION_CREDENTIALS defines the path to the JSON storing your Google Application Credentials

# GOOGLE_APPLICATION_CREDENTIALS = 

# Enable Google's extended thinking mode for better reasoning capabilities.
# Set to 1 to enable thinking mode, 0 to disable.
#GOOGLE_THINKING = 0

# Set the number of tokens allocated for Google's thinking process.
# This controls how much the model can "think" before generating the final response.
# Higher values allow for more thorough reasoning but consume more tokens.
#GOOGLE_THINKING_TOKENS = 2000 

##==============================
## Google AI Studio
## NOTE: Not intended for production.
##==============================

# GOOGLE_AI_STUDIO_API_KEY is your Google AI Studio API key

# GOOGLE_AI_STUDIO_API_KEY = 

##==============================
## Groq
##==============================

# GROQ_API_KEY defines the API key for your Groq account.

# GROQ_API_KEY = 

##==============================
## OpenRouter
##==============================

# OPENROUTER_API_KEY defines the API key for your OpenRouter account.

# OPENROUTER_API_KEY = 

# In a comma-separated list, you can specify the providers you prefer OpenRouter route your LLM calls to.
#OPENROUTER_PREFERRED_PROVIDERS = 

##==============================
## SambaNova
##==============================

# SAMBANOVA_API_KEY defines the API key for your SambaNova account.

# SAMBANOVA_API_KEY = 

##==============================
## Bedrock
##==============================

# AWS_REGION, AWS_PROFILE_NAME, AWS_ROLE_ARN, AWS_SECRET_ACCESS_KEY, and AWS_ACCESS_KEY_ID define the connection parameters to your AWS Bedrock instance.
# If using EC2 with an IAM profile, you only need to set AWS_REGION to select the region you want to deploy Bedrock in.
# This is relevant because different regions have different models/costs/latencies and it is a required parameter by AWS when making a call.

# AWS_REGION = 
# AWS_PROFILE_NAME = 
# AWS_ROLE_ARN = 
# AWS_ACCESS_KEY_ID = 
# AWS_SECRET_ACCESS_KEY = 

# By default, AWS SDK use the global STS endpoint (https://sts.amazonaws.com).
# To enable region-specific STS endpoints (e.g., https://sts.us-west-2.amazonaws.com),
# uncomment the following line:
#AWS_STS_REGIONAL_ENDPOINTS=regional

# Enable Claude's extended thinking mode for better reasoning capabilities.
# Set to 1 to enable thinking mode, 0 to disable.
# Note: Only available for specific Claude models:
#   - Claude Opus 4: anthropic.claude-opus-4-20250514-v1:0
#   - Claude Sonnet 4: anthropic.claude-sonnet-4-20250514-v1:0
#   - Claude 3.7 Sonnet: anthropic.claude-3-7-sonnet-20250219-v1:0
#AWS_CLAUDE_THINKING = 0

# Set the number of tokens allocated for Claude's thinking process.
# This controls how much the model can "think" before generating the final response.
# Higher values allow for more thorough reasoning but consume more tokens.
#AWS_CLAUDE_THINKING_TOKENS = 2000

##==============================
## Mistral
##==============================

# MISTRAL_API_KEY = 

##==============================
## NVIDIA NIM
##==============================

# NVIDIA_API_KEY = 

# If self-hosting NVIDIA NIM, set the base url here, like "http://localhost:8000/v1"
#NVIDIA_BASE_URL = 

##==============================
## Custom Providers
## You can configure any non-standard provider or multiple instances of the same provider type (e.g., two different Azure endpoints).
##==============================

# ------------------------------
# Example 1: Custom OpenAI-Compatible Provider
# ------------------------------
# Use this for any LLM that exposes an OpenAI-compatible API.
# 1. Choose a provider name, for example: MY_API
# 2. Set it in section 4, e.g., CHAT_PROVIDER = MY_API
# 3. Define its parameters, following the pattern: [PROVIDER_NAME]_API_KEY and [PROVIDER_NAME]_BASE_URL

#MY_API_API_KEY =
#MY_API_BASE_URL =

# ------------------------------
# Example 2: Custom Azure Providers
# ------------------------------
# Use this when you need separate configurations for chat and embeddings, especially if they use different API versions.
# 1. Choose provider names that MUST start with "AZURE_", for example: AZURE_CHAT and AZURE_EMBEDDING
# 2. Set them in section 4:
#    CHAT_PROVIDER = AZURE_CHAT
#    EMBEDDINGS_PROVIDER = AZURE_EMBEDDING
# 3. Define their parameters, following the pattern: [PROVIDER_NAME]_ENDPOINT, [PROVIDER_NAME]_API_VERSION, etc.

# --- Configuration for the CHAT provider ---
#AZURE_CHAT_ENDPOINT =
#AZURE_CHAT_API_KEY =
#AZURE_CHAT_API_VERSION =

# --- Configuration for the EMBEDDING provider (with a different API version) ---
#AZURE_EMBEDDING_ENDPOINT =
#AZURE_EMBEDDING_API_KEY =
#AZURE_EMBEDDING_API_VERSION =
#AZURE_EMBEDDING_EMBEDDINGS_DIMENSIONS =

## ==============================
## 6. Custom Instructions
## ==============================

# You can use CUSTOM_INSTRUCTIONS to supply the LLM with additional data it may need to correctly resolve your queries.
# This should be used as an complement to the metadata stored in Denodo, not as a substitute.
#CUSTOM_INSTRUCTIONS = 

## ==============================
## 7. DeepQuery
## WARNING: Using DeepQuery requires more powerful LLMs. You will need:
## - A thinking model with at least 128k context and 50RPM allowance
## ==============================

# DEEPQUERY_EXECUTION_MODEL defines which LLM to use for DeepQuery execution phase.
# "thinking" = Use THINKING_LLM_* for execution (recommended for best performance)
# "base" = Use LLM_* for execution (more cost-effective but may be less capable)
# Planning phase always uses THINKING_LLM_* as it requires reasoning capabilities.

# DEEPQUERY_EXECUTION_MODEL = thinking
 
# DeepQuery Configuration
# DEEPQUERY_DEFAULT_ROWS = 10
# DEEPQUERY_MAX_ANALYSIS_LOOPS = 50
# DEEPQUERY_MAX_REPORTING_LOOPS = 25
# DEEPQUERY_MAX_CONCURRENT_TOOL_CALLS = 5

# DeepQuery PDF Color Schemes
# Configuration for color schemes used in DeepQuery PDF reports.
# Each color scheme has two properties: primary and text colors.

# Blue color scheme
# DEEPQUERY_PDF_COLOR_SCHEME_BLUE_PRIMARY = #3182ce
# DEEPQUERY_PDF_COLOR_SCHEME_BLUE_TEXT = #ffffff

# Black color scheme
# DEEPQUERY_PDF_COLOR_SCHEME_BLACK_PRIMARY = #2d3748
# DEEPQUERY_PDF_COLOR_SCHEME_BLACK_TEXT = #ffffff

# Red color scheme
# DEEPQUERY_PDF_COLOR_SCHEME_RED_PRIMARY = #c53030
# DEEPQUERY_PDF_COLOR_SCHEME_RED_TEXT = #ffffff

# Green color scheme
# DEEPQUERY_PDF_COLOR_SCHEME_GREEN_PRIMARY = #38a169
# DEEPQUERY_PDF_COLOR_SCHEME_GREEN_TEXT = #ffffff